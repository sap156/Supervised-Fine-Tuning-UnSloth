{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcEz07x7d8mYyQzUjU6C+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sap156/Supervised-Fine-Tuning-UnSloth/blob/main/SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QpJfPbWTjN5A"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "BhVHyBK_oCXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "rFo-VPkloj9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HuggingFace')  # Securely fetch token from Colab secrets\n",
        "login(hf_token)  # Log in to Hugging Face"
      ],
      "metadata": {
        "id": "HWYEyNGso5Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token,\n",
        ")\n"
      ],
      "metadata": {
        "id": "_VmD3FAxtNl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "<think>{}\"\"\""
      ],
      "metadata": {
        "id": "5D9Gu7ATvn3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"A 28-year-old woman presents with a 3-week history of an intensely itchy, red, and scaly rash on the flexor surfaces of her elbows and behind her knees. She reports a personal history of asthma and seasonal allergies. Physical examination reveals lichenification and excoriations in the affected areas. What is the most likely diagnosis, and what is the primary immune pathway involved in this condition?\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        " input_ids=inputs.input_ids,\n",
        " attention_mask=inputs.attention_mask,\n",
        " max_new_tokens=1200,\n",
        " use_cache=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "id": "szSylQtNwxhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=9001,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "vMn3wH_mxzWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "<think>\n",
        "\n",
        "{}\n",
        "\n",
        "</think>\n",
        "\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "BKBi9D0oyUAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN to signal the end of each example\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Question\"]          # The medical question\n",
        "    cots = examples[\"Complex_CoT\"]         # The reasoning/explanation (Chain of Thought)\n",
        "    outputs = examples[\"Response\"]         # The final answer\n",
        "    texts = []\n",
        "\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\": texts }\n"
      ],
      "metadata": {
        "id": "v-3mJc_l0O3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the first 500 records from the dataset\n",
        "dataset = load_dataset(\n",
        "    \"FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "    \"en\",\n",
        "    split=\"train[0:500]\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Apply the formatting function to shape each example into a training prompt\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Display the formatted second example to verify\n",
        "dataset[\"text\"][1]\n"
      ],
      "metadata": {
        "id": "CIpKaLnU0Rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,  # Number of processes to load/format the dataset\n",
        "\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "\n",
        "        # Use num_train_epochs and warmup_ratio for longer runs\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,  # Keep small for quick demos. Increase for real training.\n",
        "\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),  # Use fp16 unless bf16 is supported\n",
        "        bf16=is_bfloat16_supported(),      # Use bf16 if supported by your GPU\n",
        "\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",  # 8-bit AdamW optimizer to reduce memory use\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "\n",
        "        seed=3407,  # Ensures reproducibility\n",
        "        output_dir=\"outputs\",  # Where checkpoints and logs are stored\n",
        "        report_to=\"none\"  # Disables logging to external services like WandB\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "jGRI3-d62UdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "Cb5gMU3T26qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"A 28-year-old woman presents with a 3-week history of an intensely itchy, red, and scaly rash on the flexor surfaces of her elbows and behind her knees.\n",
        "She reports a personal history of asthma and seasonal allergies. Physical examination reveals lichenification and excoriations in the affected areas.\n",
        "What is the most likely diagnosis, and what is the primary immune pathway involved in this condition?\"\"\"\n",
        "\n",
        "# Set the model to inference mode again\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the prompt using the original prompt format\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a prediction using the fine-tuned model\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "\n",
        "# Decode and display the model's new response\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "id": "qEHF7WtZDiwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_online = \"sap156/DeepSeek-R1-Medical-INSTAGRAM\"  # Hugging Face repo name\n",
        "new_model_local = \"DeepSeek-R1-Medical-INSTAGRAM\"           # Local folder name\n",
        "\n",
        "# Save model and tokenizer locally\n",
        "model.save_pretrained(new_model_local)\n",
        "tokenizer.save_pretrained(new_model_local)"
      ],
      "metadata": {
        "id": "3bW301vMDjqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hugging Face — requires a write-enabled HF token (set earlier in Step 5)\n",
        "model.push_to_hub(new_model_online)\n",
        "tokenizer.push_to_hub(new_model_online)"
      ],
      "metadata": {
        "id": "vAckmIKxDp38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}